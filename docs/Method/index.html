<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Method">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Method | Low-light Image Enhancement using Deep Learning</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://shuangshen19.github.io/CS639-Website/docs/Method"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Method | Low-light Image Enhancement using Deep Learning"><meta data-rh="true" name="description" content="In this project, we adopt KinD algorithm proposed by Zhang et al. [5] to build a low-light image emhancer. It is a CNN-based approach and the original implementation is on TensorFlow. We re-implement the algorithm by using Pytorch and do the experiment on LOLdataset [6]. In this section, we will introduce the network, training scheme, and inference procedure."><meta data-rh="true" property="og:description" content="In this project, we adopt KinD algorithm proposed by Zhang et al. [5] to build a low-light image emhancer. It is a CNN-based approach and the original implementation is on TensorFlow. We re-implement the algorithm by using Pytorch and do the experiment on LOLdataset [6]. In this section, we will introduce the network, training scheme, and inference procedure."><link data-rh="true" rel="icon" href="/CS639-Website/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://shuangshen19.github.io/CS639-Website/docs/Method"><link data-rh="true" rel="alternate" href="https://shuangshen19.github.io/CS639-Website/docs/Method" hreflang="en"><link data-rh="true" rel="alternate" href="https://shuangshen19.github.io/CS639-Website/docs/Method" hreflang="x-default"><link rel="stylesheet" href="/CS639-Website/assets/css/styles.5bd81ef5.css">
<link rel="preload" href="/CS639-Website/assets/js/runtime~main.d0d2eecf.js" as="script">
<link rel="preload" href="/CS639-Website/assets/js/main.8e0ba4c4.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/CS639-Website/"><div class="navbar__logo"><img src="/CS639-Website/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/CS639-Website/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/CS639-Website/docs/Introduction">Report</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/shuangshen19/CS639-Website" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/Introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/Related-Work">Related Work</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/CS639-Website/docs/Method">Method</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/Evaluation">Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/conclusion">Conclusion &amp; Future Work</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/project-proposal">Project Proposal</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/midterm-report">Midterm Report</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/src">Source Code &amp; Presentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/CS639-Website/docs/reference">Reference</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/CS639-Website/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Method</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Method</h1><p>In this project, we adopt KinD algorithm proposed by Zhang et al. <!-- -->[5]<!-- --> to build a low-light image emhancer. It is a CNN-based approach and the original implementation is on TensorFlow. We re-implement the algorithm by using Pytorch and do the experiment on LOLdataset <!-- -->[6]<!-- -->. In this section, we will introduce the network, training scheme, and inference procedure.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="background-retinex-theory">Background: Retinex Theory<a class="hash-link" href="#background-retinex-theory" title="Direct link to heading">​</a></h2><p>Since the method is based on retinex theory, which was proposed by Edwin H. Land in 1963 and has been applied to many image enhancement applications, we briefly introduce retinex theory here. Retinex theory indicates that an image is a composition of two components, illumination map and reflectance map. The illumination map models the strength of incident light while the reflectance map represents that reflection property of object surface. Figure 3.1 is an example of decomposing an image into illumination map and reflectance map. Since illumination map is the strength of incident light, it looks piece-wise smooth. As for reflectance map, it shows the detail and the texture of objects. Also, noise is on the reflectance map.</p><p>The intuition of using retinex theory on low-light image enhancement is that if getting illumination map and reflectance map with high quality, it would be easy to raise the brightness of image without amplifying noise. Besides, noise is extracted on reflectance map, so noise reduction is also easier.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_retinex_example-ac9b26d03a03efbd4ed2335ee2e831fb.png" width="458" height="914" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.1. Example of illumination map and reflectance map <!-- -->[5]</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="kind-network-architecture">KinD Network Architecture<a class="hash-link" href="#kind-network-architecture" title="Direct link to heading">​</a></h2><p>KinD is a CNN-based method which first decomposes input image into illumination map and reflectance map by a layer decomposition net. Then, for two maps, it further use two separate networks to complete different tasks. For reflectance map, it uses reflectance restoration net to reduce the degradation. For illumination map, it uses illumination adjustment net to modify the brightness level of illumination net. After two maps are processed, enhanced output can be simply generated by doing element-wise prodcut between reflectance map and illumination map. Figure 3.2 shows the overall architecture.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_architecture-24e5f5bf3aa43a0687f5395bd65ca5ff.png" width="1924" height="870" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.2. Architecture of KinD algorithm <!-- -->[5]</td></tr></tbody></table><p>Since there is no ground truth reflectance and illumination, the network is trained by paired high-light and low-light images. Specifically, it use high-light image to guide low-light image.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="layer-decomposition-net">Layer Decomposition Net<a class="hash-link" href="#layer-decomposition-net" title="Direct link to heading">​</a></h2><p>Layer decomposition net is the first phase of the algorithm. Its task is to learn the decomposition of retinex theory. Its inputs are a high-light image and a low-light image and its outputs are reflectance map and illumination map of high-light image and those of low-light image.</p><p>There are two branches in this network. One branch is for generating reflectance map and the other is for generating illumination map. The architecture of reflectance branch is U-Net <!-- -->[4]<!-- --> and that of illumination branch is simply 3 convolutional layers. Also, last layer of reflectance branch is passed to illumination branch for excluding texture on illumination map.</p><p>Figure 3.3 shows loss functions used in layer decomposition net. The first one wants reflectance map of high light and that of low light to be close since the scene is the same. The second one makes illumination becomes piece-wise smooth. The third one wants to preserve strong edges in both illumination maps while depress weak edges. The last one wants the reconstruction to be close to input.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_decomposition_loss-62d161e7c206788f6473b9e51c488c4b.png" width="4409" height="1562" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.3. Loss functions of layer decomposition net <!-- -->[5]</td></tr></tbody></table><p>In our implement, the weight for these loss functions are 0.009, 0.005, 0.2, and 1.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reflectance-restoration-net">Reflectance Restoration Net<a class="hash-link" href="#reflectance-restoration-net" title="Direct link to heading">​</a></h2><p>Reflectance Restoration net uses high-light reflectance map as target since it has less noise and defect. It tries to align low-light reflectance map to high-light reflectance map to remove noise and degradation. The loss functions for this goal are square l2-norm between reflectance maps of high-light and output, SSIM between reflectance maps of high-light and output, and square l2-norm between gradient of reflectance maps of high-light and output, like figure 3.4 shows.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_restoration_loss-9a4bf8feaec52079d2423e847f43c145.png" width="856" height="82" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.4. Loss functions of reflectance restoration net <!-- -->[5]</td></tr></tbody></table><p>The input of reflectance restoration net also contains illumination map because strength of degradation depends on illumination. Thus, it is helpful to include illumination map in input.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="illumination-adjustment-net">Illumination Adjustment Net<a class="hash-link" href="#illumination-adjustment-net" title="Direct link to heading">​</a></h2><p>Illumination adjustment net&#x27;s input is a
illumination map concatenated with a ratio map. Its goal is to adjust the illumination map by that ratio. During training, it randomly choose high-light or low-light illumination as input and the other as target. Then, the ratio will be average brightness of input illumination over that of target illumination. It further use squre l2-norm between target and output to constraint the training, which is shown in figure 3.5.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_adjustment_loss-fe2c033d8b536e9d877ac051506548ce.png" width="612" height="68" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.5. Loss functions of illumination adjustment net <!-- -->[5]</td></tr></tbody></table><p>During inference, user can set ratio to arbitrary value to indicate the amount of adjustment. We also show effects of different ratios in the evaluation section.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a class="hash-link" href="#training" title="Direct link to heading">​</a></h2><p>As for training, layer decomposition net is trained first. With the trained layer decomposition net, reflectance restoration net and illumination adjustment net can be trainined paralleled.</p><p>For layer decomposition net, we train it with randomly cropped 48x48 patch and use 400 epoches. For reflectance restoration net, randomly cropped patch is 128x128 and epoches is 1000. Finally, for illumination adjustment net, patch is 48x48 and epoches is 2000. All of them also include data augmentation like random flip and rotation and are trained with Adam optimizer.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference">Inference<a class="hash-link" href="#inference" title="Direct link to heading">​</a></h2><p>Figure 3.6 show the inference with real data. The ratio in this example is set to 5.</p><table><thead><tr><th align="center"><img loading="lazy" src="/CS639-Website/assets/images/kind_inference-754eba28737c789b12b08dd109fa5034.png" width="4404" height="1549" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center">Figure 3.6. Inference flow with real data</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/3-Method.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/CS639-Website/docs/Related-Work"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Related Work</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/CS639-Website/docs/Evaluation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Evaluation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#background-retinex-theory" class="table-of-contents__link toc-highlight">Background: Retinex Theory</a></li><li><a href="#kind-network-architecture" class="table-of-contents__link toc-highlight">KinD Network Architecture</a></li><li><a href="#layer-decomposition-net" class="table-of-contents__link toc-highlight">Layer Decomposition Net</a></li><li><a href="#reflectance-restoration-net" class="table-of-contents__link toc-highlight">Reflectance Restoration Net</a></li><li><a href="#illumination-adjustment-net" class="table-of-contents__link toc-highlight">Illumination Adjustment Net</a></li><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a></li><li><a href="#inference" class="table-of-contents__link toc-highlight">Inference</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Fall22 CS639-Final-Project. Built with Docusaurus.</div></div></div></footer></div>
<script src="/CS639-Website/assets/js/runtime~main.d0d2eecf.js"></script>
<script src="/CS639-Website/assets/js/main.8e0ba4c4.js"></script>
</body>
</html>